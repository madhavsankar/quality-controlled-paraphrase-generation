{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60_k0u0wob5r",
        "outputId": "17fdf9c3-481d-427f-8876-0e6095e01c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "#Importing Drive (Only for colab)\n",
        "runningOnColab = 'google.colab' in sys.modules\n",
        "if runningOnColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd '/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "os.environ['PYTHONPATH'] = cwd"
      ],
      "metadata": {
        "id": "ZypXZUm-SRFT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "db4NiTSCPPaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgLRX8fZosoK",
        "outputId": "0d8bbe9a-b37d-41cb-bf75-5f14508e97eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: transformers==4.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.9.1)\n",
            "Requirement already satisfied: sacrebleu==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: Levenshtein==0.18.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: benepar==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: apted==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: bert_score==0.3.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.3.11)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.1.96)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.79)\n",
            "Requirement already satisfied: cadences in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.3.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.7.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.12.1+cu113)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (4.13.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (4.49.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (0.3.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (2022.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.10.3)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu==1.5.1->-r requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from Levenshtein==0.18.1->-r requirements.txt (line 5)) (2.13.2)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (0.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score==0.3.11->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.38.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.50.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (2.4.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (8.1.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 14)) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->-r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar==0.2.0->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar==0.2.0->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.7.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.7.0->-r requirements.txt (line 1)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.7.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->-r requirements.txt (line 14)) (5.2.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 14)) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 14)) (0.7.9)\n",
            "Requirement already satisfied: svgling in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (0.3.1)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (5.4.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: blosc in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (1.10.6)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from cadences->-r requirements.txt (line 11)) (3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 12)) (3.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->cadences->-r requirements.txt (line 11)) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->-r requirements.txt (line 14)) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score==0.3.11->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score==0.3.11->-r requirements.txt (line 8)) (0.11.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from stanza->cadences->-r requirements.txt (line 11)) (2.2.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (from svgling->cadences->-r requirements.txt (line 11)) (1.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clearml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGrVYVXePBeN",
        "outputId": "89ab3671-fe97-4147-c3ec-0f264f24a0d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: clearml in /usr/local/lib/python3.7/dist-packages (1.7.2)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from clearml) (6.0)\n",
            "Requirement already satisfied: pathlib2>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.3.7.post1)\n",
            "Requirement already satisfied: pyjwt<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from clearml) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.21.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.24.3)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (4.3.3)\n",
            "Requirement already satisfied: furl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.1.3)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (7.1.2)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (22.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from clearml) (5.4.8)\n",
            "Requirement already satisfied: orderedmultidict>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from furl>=2.0.0->clearml) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (4.13.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6.0->clearml) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data"
      ],
      "metadata": {
        "id": "PW0cEN9aJfcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/train.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/train.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voKiL4tvtf0U",
        "outputId": "e322bc43-cf9b-4bba-bb62-089f294f0f48"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-e0c679be8369f11f\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-e0c679be8369f11f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "2022-11-10 02:04:49.013545: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Computing metric...\n",
            "bleu: 100% 10/10 [00:00<00:00, 310.85it/s]\n",
            "syntdiv:calc_dist: 100% 10/10 [00:00<00:00, 39.68it/s]\n",
            "phonemetric:find_sim: 100% 10/10 [00:02<00:00,  4.08it/s]\n",
            "morphmetric:find_sim: 100% 10/10 [00:00<00:00, 61.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/validation.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/validation.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv21OPStCpol",
        "outputId": "680697ff-c375-4345-e000-9857640f20fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-9af5fcb93faffdeb\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-9af5fcb93faffdeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9af5fcb93faffdeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "2022-11-10 01:48:50.867162: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Traceback (most recent call last):\n",
            "  File \"QCPG/evaluate.py\", line 72, in <module>\n",
            "    main()\n",
            "  File \"QCPG/evaluate.py\", line 53, in main\n",
            "    metric = load_metric(eval_args.metric_name_or_path, experiment_id=os.getpid())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 619, in load_metric\n",
            "    metric.download_and_prepare(download_config=download_config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/metric.py\", line 536, in download_and_prepare\n",
            "    self._download_and_prepare(dl_manager)\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/para_metric/7c1deea94a6b8670d57c37b99965fae6cf3d2aaaa48f3581648550fe57e1cd9e/para_metric.py\", line 78, in _download_and_prepare\n",
            "    self.phon = load_metric(\"metrics/phon_metric\", experiment_id=self.experiment_id)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 605, in load_metric\n",
            "    dataset=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 445, in prepare_module\n",
            "    f\"To be able to use this {module_type}, you need to install the following dependencies\"\n",
            "ImportError: To be able to use this metric, you need to install the following dependencies['Bio', 'cadence'] using 'pip install Bio cadence' for instance'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/test.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/test.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zQwbTNCytB",
        "outputId": "65abc756-8785-43d0-df35-e9dfc1c6a603"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-bb2bde7c7bb15efd\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-bb2bde7c7bb15efd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bb2bde7c7bb15efd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "2022-11-10 01:49:10.959191: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Traceback (most recent call last):\n",
            "  File \"QCPG/evaluate.py\", line 72, in <module>\n",
            "    main()\n",
            "  File \"QCPG/evaluate.py\", line 53, in main\n",
            "    metric = load_metric(eval_args.metric_name_or_path, experiment_id=os.getpid())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 619, in load_metric\n",
            "    metric.download_and_prepare(download_config=download_config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/metric.py\", line 536, in download_and_prepare\n",
            "    self._download_and_prepare(dl_manager)\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/para_metric/7c1deea94a6b8670d57c37b99965fae6cf3d2aaaa48f3581648550fe57e1cd9e/para_metric.py\", line 78, in _download_and_prepare\n",
            "    self.phon = load_metric(\"metrics/phon_metric\", experiment_id=self.experiment_id)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 605, in load_metric\n",
            "    dataset=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 445, in prepare_module\n",
            "    f\"To be able to use this {module_type}, you need to install the following dependencies\"\n",
            "ImportError: To be able to use this metric, you need to install the following dependencies['Bio', 'cadence'] using 'pip install Bio cadence' for instance'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/train.py --model_name_or_path t5-base \\\n",
        "--do_train --do_eval --source_column reference \\\n",
        "--target_column prediction --per_device_eval_batch_size 16 \\\n",
        "--per_device_train_batch_size 16 --predict_with_generate \\\n",
        "--evaluation_strategy epoch --num_train_epochs 6 \\\n",
        "--lr_scheduler_type constant --save_total_limit 1 \\\n",
        "--dataset_generate_mode force_redownload --dataset_keep_in_memory \\\n",
        "--conditions_columns '[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]' \\\n",
        "--overwrite_output_dir \\\n",
        "--dataset_map 'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);' \\\n",
        "--train_file new_data/mscoco/train.csv.gz \\\n",
        "--validation_file new_data/mscoco/validation.csv.gz \\\n",
        "--learning_rate 1e-3 \\\n",
        "--output_dir new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1 \\\n",
        "--dataset_generate_mode force_redownload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAuFlS5VC5M8",
        "outputId": "86aab1fc-bc05-4824-ba6a-89995ae59f10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with `--source_prefix 'summarize: ' `\n",
            "WARNING:__main__:Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov10_01-49-27_a8ba82c87e0e', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "WARNING:datasets.builder:Using custom data configuration default-40b3d770121fb9c7\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-40b3d770121fb9c7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-40b3d770121fb9c7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function prepare_dataset.<locals>.<lambda> at 0x7fdc90303c20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 10/10 [00:00<00:00, 3744.58ex/s]\n",
            "100% 10/10 [00:00<00:00, 3892.99ex/s]\n",
            "INFO:__main__:Dataset was mapped with 'None'\n",
            "INFO:__main__:Full conditions list: {all_conditions}\n",
            "https://huggingface.co/t5-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi38_3nl3\n",
            "Downloading: 100% 1.20k/1.20k [00:00<00:00, 779kB/s]\n",
            "storing https://huggingface.co/t5-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "creating metadata file for /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-base/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7z1jotnr\n",
            "Downloading: 100% 792k/792k [00:00<00:00, 6.64MB/s]\n",
            "storing https://huggingface.co/t5-base/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "https://huggingface.co/t5-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf1fzs1rt\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 11.1MB/s]\n",
            "storing https://huggingface.co/t5-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "creating metadata file for /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu2us0rof\n",
            "Downloading: 100% 892M/892M [00:14<00:00, 60.0MB/s]\n",
            "storing https://huggingface.co/t5-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
            "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 73.74ba/s]\n",
            "100% 1/1 [00:00<00:00, 108.48ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n",
            " 17% 1/6 [00:14<01:14, 14.98s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0     3 19698   229     3    15     7     3     9   720     3    26\n",
            "     60  1208     6    68    34    19     3     9]\n",
            " [    0     3 25048   222  5288     7    30     3     9 27094   344     3\n",
            "      9   508   740    11     3     9  4033     5]\n",
            " [    0     3 10997 10997 10997 10997 10997 10997 10997 10997 10997 10997\n",
            "  10997 10997 10997 10997 10997 10997 10997 10997]\n",
            " [    0     3     9  5065  6702  7293    28     3     9   643    13   387\n",
            "   1084    57     5     1     0     0     0     0]\n",
            " [    0 32099  1458 32098     3 32097     3 32096     3 32095   834  2445\n",
            "  32094   834  2445 32093    11  3285     5 32092]\n",
            " [    0     3   632  8472   308   834  3765     4 23936   834   308  7589\n",
            "    834  1458   375   929     3    32     3     9]\n",
            " [    0     3 21757     3     9   563    13   151  3823    44     3     9\n",
            "    953    28  9832    13  2013     5     1     0]\n",
            " [    0     3     9   563    13   151  3281    45  2013  9832     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     3     9   563    13   151  2013 12246    44     3     9   953\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0     3     9   563    13   151  3823    44     3     9   953    28\n",
            "   9832    13  2013     5     1     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.644071578979492, 'eval_bleu': 6.5029, 'eval_gen_len': 16.8, 'eval_runtime': 20.3715, 'eval_samples_per_second': 0.491, 'epoch': 1.0}\n",
            " 17% 1/6 [00:35<01:14, 14.98s/it]\n",
            "100% 1/1 [00:00<00:00, 75.32it/s]\u001b[A\n",
            " 33% 2/6 [00:48<01:21, 20.46s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0     3     9  5065  7293    28     3     9   643    13   387  1084\n",
            "     57     3     9  5065   387  1451     5     1]\n",
            " [    0     3 25048   222  5288     7    30     3     9 27094   344     3\n",
            "      9   508   740    11     3     9  4033     5]\n",
            " [    0    71     3  8343    28   151  3214    11     3     9  6702  7293\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0     3     9  5065  6702  7293    28     3     9   643    13   387\n",
            "   1084    57     3     9  5065   387  1451     3]\n",
            " [    0     3     5     3     5     3     5     3     5     3     5     3\n",
            "      5     3     5     3     5     3     5     3]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3281    45  2013  9832     5     1     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  2013 12246    44     3     9   953     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.4681034088134766, 'eval_bleu': 13.2551, 'eval_gen_len': 15.6, 'eval_runtime': 23.6808, 'eval_samples_per_second': 0.422, 'epoch': 2.0}\n",
            " 33% 2/6 [01:11<01:21, 20.46s/it]\n",
            "100% 1/1 [00:00<00:00, 93.57it/s]\u001b[A\n",
            " 50% 3/6 [01:24<01:15, 25.29s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  5065     6     3     7  8745    53     6     3 23164     6\n",
            "      3 23164     6     3 23164     6     3 23164]\n",
            " [    0    71 16400 10681   344     3     9   508   740    11     3     9\n",
            "   4033     5     1     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9   360   151  3214   300    34\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71  5065     6     3     7 26605     6    11     3     7 26605\n",
            "   6702  7293    28     3     9   643    13   387]\n",
            " [    0     3     2     3     2     3     2     3     2     3     2     3\n",
            "      2     3     2     3     2     3     2     3]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3281    45  2013  9832     5     1     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  2013 12246    44     3     9   953     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.5189900398254395, 'eval_bleu': 13.7238, 'eval_gen_len': 15.1, 'eval_runtime': 20.4893, 'eval_samples_per_second': 0.488, 'epoch': 3.0}\n",
            " 50% 3/6 [01:45<01:15, 25.29s/it]\n",
            "100% 1/1 [00:00<00:00, 89.04it/s]\u001b[A\n",
            " 67% 4/6 [01:57<00:55, 27.64s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71   422     6    68   786     6     3 24867    11   168 13264\n",
            "    629    28     3     9   360   422  3196    11]\n",
            " [    0    71 16400     7    31  1482   190     3     9   508   740     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9   360   151  3214   300    34\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71   422     6    68   786     6     3 24867    11   168 13264\n",
            "    629    28     3     9  2201    13   387    11]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "     44     3     9  2013   651    16     8   690]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151   113  3281    45  9832    13  2013     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720    13     3\n",
            "      9     3   687    26    63     3     7    29]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28     3\n",
            "      9  1905    13  2013     5     1     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.5975289344787598, 'eval_bleu': 14.4699, 'eval_gen_len': 16.0, 'eval_runtime': 21.5497, 'eval_samples_per_second': 0.464, 'epoch': 4.0}\n",
            " 67% 4/6 [02:19<00:55, 27.64s/it]\n",
            "100% 1/1 [00:00<00:00, 93.58it/s]\u001b[A\n",
            " 83% 5/6 [02:32<00:29, 29.71s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  4716    24  1979     7    12     3     9  4033     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9  4947    13 27026    11  1692\n",
            "   2383    30     8  4205   756    34     5     1]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151   113  3281    45  2013  9832     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720    13     3\n",
            "      9     3     7    29    32   115    30     8]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28     3\n",
            "      9  4947    13 27026    11  1692  2383    30]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7148728370666504, 'eval_bleu': 12.9204, 'eval_gen_len': 15.6, 'eval_runtime': 23.1719, 'eval_samples_per_second': 0.432, 'epoch': 5.0}\n",
            " 83% 5/6 [02:55<00:29, 29.71s/it]\n",
            "100% 1/1 [00:00<00:00, 87.95it/s]\u001b[A\n",
            "100% 6/6 [03:08<00:00, 31.56s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  4716    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "     44     3     9  2062     5     1     0     0]\n",
            " [    0    71   563    13   151    28     3     9  4947    13 27026    11\n",
            "   1692  2383    30     8  4205   756   135     5]\n",
            " [    0    71   563    13   151    28     3     9  4947    13 27026    11\n",
            "   1692  2383    30     8  4205   756   135     5]\n",
            " [    0    71   563    13   151   113  3281    45     3     9  1905    13\n",
            "   2013     5     1     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720   315    45\n",
            "    284   119    33  2013 12246    44     3     9]\n",
            " [    0    71   563    13   151   113   698     3     9   953    28   284\n",
            "    119     5     1     0     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.961228847503662, 'eval_bleu': 11.7334, 'eval_gen_len': 17.0, 'eval_runtime': 22.6041, 'eval_samples_per_second': 0.442, 'epoch': 6.0}\n",
            "100% 6/6 [03:30<00:00, 31.56s/it]\n",
            "100% 1/1 [00:00<00:00, 55.07it/s]\u001b[A\n",
            "                                 \u001b[A\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 211.001, 'train_samples_per_second': 0.028, 'epoch': 6.0}\n",
            "100% 6/6 [03:30<00:00, 35.16s/it]\n",
            "Saving model checkpoint to new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1\n",
            "Configuration saved in new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/config.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-60ba11ecd789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python QCPG/train.py --model_name_or_path t5-base  --do_train --do_eval --source_column reference  --target_column prediction --per_device_eval_batch_size 16  --per_device_train_batch_size 16 --predict_with_generate  --evaluation_strategy epoch --num_train_epochs 6  --lr_scheduler_type constant --save_total_limit 1  --dataset_generate_mode force_redownload --dataset_keep_in_memory  --conditions_columns \\'[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]\\'  --overwrite_output_dir  --dataset_map \\'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);\\'  --train_file new_data/mscoco/train.csv.gz  --validation_file new_data/mscoco/validation.csv.gz  --learning_rate 1e-3  --output_dir new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1  --dataset_generate_mode force_redownload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   result = _run_command(\n\u001b[0;32m--> 437\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/predict.py \\\n",
        "--per_device_eval_batch_size 256 --per_device_train_batch_size 256 \\\n",
        "--source_column reference --target_column prediction \\\n",
        "--conditions_columns '[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]' \\\n",
        "--dataset_map 'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);' \\\n",
        "--train_file new_data/mscoco/validation.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--model_name_or_path new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1 \\\n",
        "--output_dir new_data/validation/t5-base-cond-mscoco-bleurt-lr1e-3-v1"
      ],
      "metadata": {
        "id": "FthAYJXBC9RB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}