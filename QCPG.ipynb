{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60_k0u0wob5r",
        "outputId": "c21b2466-4f55-4f3d-b1f8-37ac016dd394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation\n",
            "/env/python:/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "#Importing Drive\n",
        "runningOnColab = 'google.colab' in sys.modules\n",
        "! echo $PYTHONPATH\n",
        "if runningOnColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd '/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation'\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/Shareddrives/Capstone/quality-controlled-paraphrase-generation\"\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GgLRX8fZosoK",
        "outputId": "28c62904-368f-481b-9884-ecc73ddee76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==1.7.0\n",
            "  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting transformers==4.6.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.9.1)\n",
            "Collecting sacrebleu==1.5.1\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting Levenshtein==0.18.1\n",
            "  Downloading Levenshtein-0.18.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 49.5 MB/s \n",
            "\u001b[?25hCollecting benepar==0.2.0\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "Collecting apted==1.0.3\n",
            "  Downloading apted-1.0.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting bert_score==0.3.11\n",
            "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (2022.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (0.3.6)\n",
            "Collecting pyarrow<4.0.0,>=1.0.0\n",
            "  Downloading pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.7.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 37.7 MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting rapidfuzz<3.0.0,>=2.0.1\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (1.12.1+cu113)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar==0.2.0->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score==0.3.11->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.50.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.38.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.7.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.7.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar==0.2.0->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar==0.2.0->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.7.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.7.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.7.0->-r requirements.txt (line 1)) (2022.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.5.0->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.7.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (3.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (1.10.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (5.2.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (0.0.3)\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 41.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.23.0-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 41.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 45.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 38.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 40.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 52.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 39.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 42.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 34.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 49.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 47.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 50.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.3-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 45.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 40.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 51.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 49.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 40.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 49.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 42.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 42.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 45.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 48.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 38.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 52.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 46.8 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 43.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 59.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 49.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 42.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 44.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.8 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 38.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 47.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 44.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 45.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 46.8 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 48.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 47.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 43.8 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 51.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 47.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 46.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0.9->benepar==0.2.0->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score==0.3.11->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score==0.3.11->-r requirements.txt (line 8)) (0.11.0)\n",
            "Building wheels for collected packages: benepar, sacremoses\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37648 sha256=9e937e4dc8213112c82c56714f2db80cc82c6a3a683e6a3c7b85a74d5072721f\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6f/a3/4d27ce92766bdedd2cbbbedb8857fb7a53534331191cda4994\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=a11f63c294768cb99559d1aeee43fb23a1c4ecbc847fa800d70d3962bb4b31c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built benepar sacremoses\n",
            "Installing collected packages: tqdm, tokenizers, sacremoses, huggingface-hub, transformers, xxhash, torch-struct, sentencepiece, rapidfuzz, pyarrow, portalocker, multiprocess, sacrebleu, Levenshtein, datasets, bert-score, benepar, apted\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "Successfully installed Levenshtein-0.18.1 apted-1.0.3 benepar-0.2.0 bert-score-0.3.11 datasets-1.7.0 huggingface-hub-0.0.8 multiprocess-0.70.14 portalocker-2.0.0 pyarrow-3.0.0 rapidfuzz-2.13.2 sacrebleu-1.5.1 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-struct-0.5 tqdm-4.49.0 transformers-4.6.1 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clearml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGrVYVXePBeN",
        "outputId": "920d14b1-ce46-4232-d1d0-9fbf31b1f39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clearml\n",
            "  Downloading clearml-1.7.2-py2.py3-none-any.whl (950 kB)\n",
            "\u001b[K     |████████████████████████████████| 950 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from clearml) (5.4.8)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (4.3.3)\n",
            "Collecting furl>=2.0.0\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.24.3)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (7.1.2)\n",
            "Collecting pyjwt<2.5.0,>=2.4.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from clearml) (3.0.9)\n",
            "Collecting pathlib2>=2.3.0\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (22.1.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from clearml) (6.0)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (0.19.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (4.13.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (5.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6.0->clearml) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2.10)\n",
            "Installing collected packages: orderedmultidict, pyjwt, pathlib2, furl, clearml\n",
            "Successfully installed clearml-1.7.2 furl-2.1.3 orderedmultidict-1.0.1 pathlib2-2.3.7.post1 pyjwt-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data"
      ],
      "metadata": {
        "id": "PW0cEN9aJfcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/train.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/train.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voKiL4tvtf0U",
        "outputId": "88840838-f932-4a05-dfdc-07c4eec267ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-e0c679be8369f11f\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-e0c679be8369f11f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "2022-11-09 20:15:26.725049: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Computing metric...\n",
            "bleu: 100% 10/10 [00:00<00:00, 226.13it/s]\n",
            "syntdiv:calc_dist: 100% 10/10 [00:00<00:00, 281.47it/s]\n",
            "phonemetric:find_sim: 100% 10/10 [00:02<00:00,  3.55it/s]\n",
            "morphmetric:find_sim: 100% 10/10 [00:00<00:00, 49.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/validation.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/validation.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv21OPStCpol",
        "outputId": "0e1ad6fe-09c7-4820-f938-1ac025fac1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-9af5fcb93faffdeb\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-9af5fcb93faffdeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "2022-11-09 20:18:23.808983: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Computing metric...\n",
            "bleu: 100% 10/10 [00:00<00:00, 264.25it/s]\n",
            "syntdiv:calc_dist: 100% 10/10 [00:00<00:00, 266.34it/s]\n",
            "phonemetric:find_sim: 100% 10/10 [00:02<00:00,  3.68it/s]\n",
            "morphmetric:find_sim: 100% 10/10 [00:00<00:00, 43.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/evaluate.py \\\n",
        "--train_file data/mscoco/test.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--predictions_column source \\\n",
        "--references_column target \\\n",
        "--metric metrics/para_metric \\\n",
        "--output_path new_data/mscoco/test.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zQwbTNCytB",
        "outputId": "a6dbb315-14ca-4a2b-e588-6d65832358d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-bb2bde7c7bb15efd\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bb2bde7c7bb15efd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "2022-11-09 20:19:32.384367: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "Computing metric...\n",
            "bleu: 100% 10/10 [00:00<00:00, 293.10it/s]\n",
            "syntdiv:calc_dist: 100% 10/10 [00:00<00:00, 286.06it/s]\n",
            "phonemetric:find_sim: 100% 10/10 [00:02<00:00,  3.76it/s]\n",
            "morphmetric:find_sim: 100% 10/10 [00:00<00:00, 55.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/train.py --model_name_or_path t5-base \\\n",
        "--do_train --do_eval --source_column reference \\\n",
        "--target_column prediction --per_device_eval_batch_size 16 \\\n",
        "--per_device_train_batch_size 16 --predict_with_generate \\\n",
        "--evaluation_strategy epoch --num_train_epochs 6 \\\n",
        "--lr_scheduler_type constant --save_total_limit 1 \\\n",
        "--dataset_generate_mode force_redownload --dataset_keep_in_memory \\\n",
        "--conditions_columns '[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]' \\\n",
        "--overwrite_output_dir \\\n",
        "--dataset_map 'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);' \\\n",
        "--train_file new_data/mscoco/train.csv.gz \\\n",
        "--validation_file new_data/mscoco/validation.csv.gz \\\n",
        "--learning_rate 1e-3 \\\n",
        "--output_dir new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1 \\\n",
        "--dataset_generate_mode force_redownload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAuFlS5VC5M8",
        "outputId": "6356969a-cb56-453f-c7a8-68d20ec4b081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with `--source_prefix 'summarize: ' `\n",
            "WARNING:__main__:Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov09_20-22-55_caca70d46595', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "WARNING:datasets.builder:Using custom data configuration default-40b3d770121fb9c7\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-40b3d770121fb9c7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "\r0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "\r                            \r\r0 tables [00:00, ? tables/s]\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-40b3d770121fb9c7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function prepare_dataset.<locals>.<lambda> at 0x7f5bbb89e680> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\r  0% 0/10 [00:00<?, ?ex/s]\r100% 10/10 [00:00<00:00, 2293.47ex/s]\n",
            "\r  0% 0/10 [00:00<?, ?ex/s]\r100% 10/10 [00:00<00:00, 3578.45ex/s]\n",
            "INFO:__main__:Dataset was mapped with 'None'\n",
            "INFO:__main__:Full conditions list: {all_conditions}\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 48.39ba/s]\n",
            "100% 1/1 [00:00<00:00, 66.19ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n",
            " 17% 1/6 [00:17<01:29, 17.82s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0     3 19698   229     3    15     7     3     9   720     3    26\n",
            "     60  1208     6    68    34    19     3     9]\n",
            " [    0     3 25048   222  5288     7    30     3     9 27094   344     3\n",
            "      9   508   740    11     3     9  4033     5]\n",
            " [    0     3 10997 10997 10997 10997 10997 10997 10997 10997 10997 10997\n",
            "  10997 10997 10997 10997 10997 10997 10997 10997]\n",
            " [    0     3     9  5065  6702  7293    28     3     9   643    13   387\n",
            "   1084    57     5     1     0     0     0     0]\n",
            " [    0 32099  1458 32098     3 32097     3 32096     3 32095   834  2445\n",
            "  32094   834  2445 32093    11  3285     5 32092]\n",
            " [    0     3   632  8472   308   834  3765     4 23936   834   308  7589\n",
            "    834  1458   375   929     3    32     3     9]\n",
            " [    0     3 21757     3     9   563    13   151  3823    44     3     9\n",
            "    953    28  9832    13  2013     5     1     0]\n",
            " [    0     3     9   563    13   151  3281    45  2013  9832     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     3     9   563    13   151  2013 12246    44     3     9   953\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0     3     9   563    13   151  3823    44     3     9   953    28\n",
            "   9832    13  2013     5     1     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.644071578979492, 'eval_bleu': 6.5029, 'eval_gen_len': 16.8, 'eval_runtime': 23.0503, 'eval_samples_per_second': 0.434, 'epoch': 1.0}\n",
            " 17% 1/6 [00:40<01:29, 17.82s/it]\n",
            "100% 1/1 [00:00<00:00, 49.11it/s]\u001b[A\n",
            " 33% 2/6 [00:54<01:34, 23.56s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0     3     9  5065  7293    28     3     9   643    13   387  1084\n",
            "     57     3     9  5065   387  1451     5     1]\n",
            " [    0     3 25048   222  5288     7    30     3     9 27094   344     3\n",
            "      9   508   740    11     3     9  4033     5]\n",
            " [    0    71     3  8343    28   151  3214    11     3     9  6702  7293\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0     3     9  5065  6702  7293    28     3     9   643    13   387\n",
            "   1084    57     3     9  5065   387  1451     3]\n",
            " [    0     3     5     3     5     3     5     3     5     3     5     3\n",
            "      5     3     5     3     5     3     5     3]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3281    45  2013  9832     5     1     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  2013 12246    44     3     9   953     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.4681034088134766, 'eval_bleu': 13.2551, 'eval_gen_len': 15.6, 'eval_runtime': 21.1327, 'eval_samples_per_second': 0.473, 'epoch': 2.0}\n",
            " 33% 2/6 [01:15<01:34, 23.56s/it]\n",
            "100% 1/1 [00:00<00:00, 85.05it/s]\u001b[A\n",
            " 50% 3/6 [01:28<01:20, 26.70s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  5065     6     3     7  8745    53     6     3 23164     6\n",
            "      3 23164     6     3 23164     6     3 23164]\n",
            " [    0    71 16400 10681   344     3     9   508   740    11     3     9\n",
            "   4033     5     1     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9   360   151  3214   300    34\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71  5065     6     3     7 26605     6    11     3     7 26605\n",
            "   6702  7293    28     3     9   643    13   387]\n",
            " [    0     3     2     3     2     3     2     3     2     3     2     3\n",
            "      2     3     2     3     2     3     2     3]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3281    45  2013  9832     5     1     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  2013 12246    44     3     9   953     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.5189900398254395, 'eval_bleu': 13.7238, 'eval_gen_len': 15.1, 'eval_runtime': 21.1587, 'eval_samples_per_second': 0.473, 'epoch': 3.0}\n",
            " 50% 3/6 [01:49<01:20, 26.70s/it]\n",
            "100% 1/1 [00:00<00:00, 89.57it/s]\u001b[A\n",
            " 67% 4/6 [02:02<00:57, 28.87s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71   422     6    68   786     6     3 24867    11   168 13264\n",
            "    629    28     3     9   360   422  3196    11]\n",
            " [    0    71 16400     7    31  1482   190     3     9   508   740     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9   360   151  3214   300    34\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71   422     6    68   786     6     3 24867    11   168 13264\n",
            "    629    28     3     9  2201    13   387    11]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "     44     3     9  2013   651    16     8   690]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151   113  3281    45  9832    13  2013     5\n",
            "      1     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720    13     3\n",
            "      9     3   687    26    63     3     7    29]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28     3\n",
            "      9  1905    13  2013     5     1     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.5975289344787598, 'eval_bleu': 14.4699, 'eval_gen_len': 16.0, 'eval_runtime': 21.1254, 'eval_samples_per_second': 0.473, 'epoch': 4.0}\n",
            " 67% 4/6 [02:23<00:57, 28.87s/it]\n",
            "100% 1/1 [00:00<00:00, 85.57it/s]\u001b[A\n",
            " 83% 5/6 [02:36<00:30, 30.37s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  4716    24  1979     7    12     3     9  4033     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71     3  8343    28     3     9  4947    13 27026    11  1692\n",
            "   2383    30     8  4205   756    34     5     1]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "      5     1     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28  9832\n",
            "     13  2013     5     1     0     0     0     0]\n",
            " [    0    71   563    13   151   113  3281    45  2013  9832     5     1\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720    13     3\n",
            "      9     3     7    29    32   115    30     8]\n",
            " [    0    71   563    13   151  3823    44     3     9   953    28     3\n",
            "      9  4947    13 27026    11  1692  2383    30]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7148728370666504, 'eval_bleu': 12.9204, 'eval_gen_len': 15.6, 'eval_runtime': 21.5159, 'eval_samples_per_second': 0.465, 'epoch': 5.0}\n",
            " 83% 5/6 [02:58<00:30, 30.37s/it]\n",
            "100% 1/1 [00:00<00:00, 86.27it/s]\u001b[A\n",
            "100% 6/6 [03:11<00:00, 31.79s/it]***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[[    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  4716    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71  6702    24  4764     7    16    28     8  1481  5168     7\n",
            "     16     3     9  2582     5     1     0     0]\n",
            " [    0    71   563    13   151   113    33    81    12   456  3182  2634\n",
            "     44     3     9  2062     5     1     0     0]\n",
            " [    0    71   563    13   151    28     3     9  4947    13 27026    11\n",
            "   1692  2383    30     8  4205   756   135     5]\n",
            " [    0    71   563    13   151    28     3     9  4947    13 27026    11\n",
            "   1692  2383    30     8  4205   756   135     5]\n",
            " [    0    71   563    13   151   113  3281    45     3     9  1905    13\n",
            "   2013     5     1     0     0     0     0     0]\n",
            " [    0    71   563    13   151   113    33     3     9   720   315    45\n",
            "    284   119    33  2013 12246    44     3     9]\n",
            " [    0    71   563    13   151   113   698     3     9   953    28   284\n",
            "    119     5     1     0     0     0     0     0]]\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.961228847503662, 'eval_bleu': 11.7334, 'eval_gen_len': 17.0, 'eval_runtime': 21.0967, 'eval_samples_per_second': 0.474, 'epoch': 6.0}\n",
            "100% 6/6 [03:32<00:00, 31.79s/it]\n",
            "100% 1/1 [00:00<00:00, 82.99it/s]\u001b[A\n",
            "                                 \u001b[A\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 212.8885, 'train_samples_per_second': 0.028, 'epoch': 6.0}\n",
            "100% 6/6 [03:32<00:00, 35.47s/it]\n",
            "Saving model checkpoint to new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1\n",
            "Configuration saved in new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/config.json\n",
            "Model weights saved in new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/pytorch_model.bin\n",
            "tokenizer config file saved in new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/tokenizer_config.json\n",
            "Special tokens file saved in new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/special_tokens_map.json\n",
            "Copy vocab file to new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =        6.0\n",
            "  init_mem_cpu_alloc_delta   =        0MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =     2414MB\n",
            "  train_mem_cpu_peaked_delta =        0MB\n",
            "  train_runtime              = 0:03:32.88\n",
            "  train_samples              =         10\n",
            "  train_samples_per_second   =      0.028\n",
            "INFO:__main__:*** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 16\n",
            "  0% 0/1 [00:00<?, ?it/s][[   0   71 6702 ...    0    0    0]\n",
            " [   0   71 4716 ...    0    0    0]\n",
            " [   0   71 6702 ...    0    0    0]\n",
            " ...\n",
            " [   0   71  563 ...    0    0    0]\n",
            " [   0   71  563 ...    0    0    0]\n",
            " [   0   71  563 ...    0    0    0]]\n",
            "100% 1/1 [00:00<00:00, 24.51it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        6.0\n",
            "  eval_bleu                 =    12.0771\n",
            "  eval_gen_len              =       17.5\n",
            "  eval_loss                 =     2.9612\n",
            "  eval_mem_cpu_alloc_delta  =        0MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_runtime              = 0:00:28.83\n",
            "  eval_samples              =         10\n",
            "  eval_samples_per_second   =      0.347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/predict.py \\\n",
        "--per_device_eval_batch_size 256 --per_device_train_batch_size 256 \\\n",
        "--source_column reference --target_column prediction \\\n",
        "--conditions_columns '[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]' \\\n",
        "--dataset_map 'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);' \\\n",
        "--train_file new_data/mscoco/validation.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--model_name_or_path new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1 \\\n",
        "--output_dir new_data/validation/t5-base-cond-mscoco-bleurt-lr1e-3-v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FthAYJXBC9RB",
        "outputId": "bd8f2dd5-da7a-41e5-92fc-4457461a62c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='new_data/validation/t5-base-cond-mscoco-bleurt-lr1e-3-v1', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=256, per_device_eval_batch_size=256, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov09_20-28-52_caca70d46595', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='new_data/validation/t5-base-cond-mscoco-bleurt-lr1e-3-v1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "WARNING:__main__:Dataset split name: 'train' treated as string. if you want to use json make sure it can be parsed proprly.\n",
            "INFO:__main__:Dataset is splitted by 'train'\n",
            "WARNING:datasets.builder:Using custom data configuration default-cb293a91e5a95a7b\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-cb293a91e5a95a7b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "\r0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-cb293a91e5a95a7b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);\n",
            "100% 10/10 [00:00<00:00, 1476.45ex/s]\n",
            "INFO:__main__:Dataset was mapped with 'None'\n",
            "INFO:__main__:Full conditions list: ['COND_LEXICAL_DIV_15', 'COND_LEXICAL_DIV_25', 'COND_LEXICAL_DIV_30', 'COND_MORPHOLOGICAL_DIV_40', 'COND_MORPHOLOGICAL_DIV_50', 'COND_MORPHOLOGICAL_DIV_55', 'COND_MORPHOLOGICAL_DIV_70', 'COND_PHONOLOGICAL_DIV_50', 'COND_PHONOLOGICAL_DIV_60', 'COND_SEMANTIC_SIM_10', 'COND_SEMANTIC_SIM_20', 'COND_SEMANTIC_SIM_25', 'COND_SEMANTIC_SIM_30', 'COND_SEMANTIC_SIM_35', 'COND_SEMANTIC_SIM_5', 'COND_SEMANTIC_SIM_50', 'COND_SEMANTIC_SIM_55', 'COND_SEMANTIC_SIM_60', 'COND_SYNTACTIC_DIV_15', 'COND_SYNTACTIC_DIV_25', 'COND_SYNTACTIC_DIV_30', 'COND_SYNTACTIC_DIV_40', 'COND_SYNTACTIC_DIV_60']\n",
            "loading configuration file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32120\n",
            "}\n",
            "\n",
            "loading file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/spiece.model\n",
            "loading file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/tokenizer.json\n",
            "loading file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/added_tokens.json\n",
            "loading file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/special_tokens_map.json\n",
            "loading file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/tokenizer_config.json\n",
            "loading configuration file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32120\n",
            "}\n",
            "\n",
            "loading configuration file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32120\n",
            "}\n",
            "\n",
            "loading weights file new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 101.11ba/s]\n",
            "INFO:__main__:*** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 10\n",
            "  Batch size = 256\n",
            "100% 1/1 [00:00<00:00,  4.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python QCPG/predict.py \\\n",
        "--per_device_eval_batch_size 256 --per_device_train_batch_size 256 \\\n",
        "--source_column reference --target_column prediction \\\n",
        "--conditions_columns '[\"semantic_sim\", \"lexical_div\", \"syntactic_div\", \"phonological_div\", \"morphological_div\"]' \\\n",
        "--dataset_map 'semantic_sim = 5 * round(bleurt_score * 100 / 5); lexical_div = 5 * round(set_diversity * 100 / 5); syntactic_div = 5 * round(syn_diversity * 100 / 5); phonological_div = 5 * round(phon_diversity * 100 / 5); morphological_div = 5 * round(morph_diversity * 100 / 5);' \\\n",
        "--train_file new_data/mscoco/validation.csv.gz \\\n",
        "--dataset_split train \\\n",
        "--model_name_or_path new_data/t5-base-cond-mscoco-bleurt-lr1e-3-v1 \\\n",
        "--output_dir new_data/validation/t5-base-cond-mscoco-bleurt-lr1e-3-v1"
      ],
      "metadata": {
        "id": "3qi1mcuFFwqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "class QualityControlPipeline:\n",
        "    \n",
        "    def __init__(self, type):\n",
        "        assert type in ['captions', 'questions', 'sentences']\n",
        "        self.pipe = pipeline('text2text-generation', model=f'ibm/qcpg-{type}')\n",
        "        self.ranges = {\n",
        "            'captions': {'lex': [0, 90], 'syn': [0, 80], 'sem': [0, 95]},\n",
        "            'sentences': {'lex': [0, 100], 'syn': [0, 80], 'sem': [0, 95]},\n",
        "            'questions': {'lex': [0, 90], 'syn': [0, 75], 'sem': [0, 95]}\n",
        "        }[type]\n",
        "\n",
        "    def __call__(self, text, lexical, syntactic, semantic, **kwargs):\n",
        "        assert all([0 <= val <= 1 for val in [lexical, syntactic, semantic]]), \\\n",
        "                 f' control values must be between 0 and 1, got {lexical}, {syntactic}, {semantic}'\n",
        "        names = ['semantic_sim', 'lexical_div', 'syntactic_div']\n",
        "        control = [int(5 * round(val * 100 / 5)) for val in [semantic, lexical, syntactic]]\n",
        "        control ={name: max(min(val , self.ranges[name[:3]][1]), self.ranges[name[:3]][0]) for name, val in zip(names, control)}\n",
        "        control = [f'COND_{name.upper()}_{control[name]}' for name in names]\n",
        "        assert all(cond in self.pipe.tokenizer.additional_special_tokens for cond in control)\n",
        "        text = ' '.join(control) + text if isinstance(text, str) else [' '.join(control) for t in text]\n",
        "        return self.pipe(text, **kwargs)\n",
        "model = QualityControlPipeline('sentences')\n",
        "model('Is this going to work or what are we doing here?', lexical=0.3, syntactic=0.5, semantic=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SYcUuSdwGb39",
        "outputId": "5c769ab7-ca8c-4e10-95a7-fe5d7f9ee54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Will it work or what is it we're doing?\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}